{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXDlXbIdEaWu"
   },
   "source": [
    "### Step 0 - Import & Install Reqd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "13NyYu2E-ZZd"
   },
   "outputs": [],
   "source": [
    "import gym # OpenAI Gym for our FrozenLake Environment\n",
    "import numpy as np # Numpy for our Qtable And Other Things\n",
    "import random # Random to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxBFKW9DBVWq",
    "outputId": "56bbd471-e3cb-4955-ca9e-7cf3e78b625f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[toy_text]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV6IDN79BnSS",
    "outputId": "746162b8-9242-478a-d674-6c8068d6211b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Surface(640x480x32 SW)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import pygame\n",
    "pygame.display.set_mode((640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0rstBQiCN0z",
    "outputId": "fda5f4c0-1522-4ca2-e73e-21770b3e004b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1024, 768)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.init()\n",
    "pygame.display.list_modes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2tKs-3KEliV"
   },
   "source": [
    "### Step 1 : Play With Frozen Lake Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5wU8gDCFvU6"
   },
   "source": [
    "Frozen Lake Environment : Navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).\n",
    "\n",
    "We can have two sizes of environment:\n",
    "\n",
    "* map_name=\"4x4\": a 4x4 grid version\n",
    "* map_name=\"8x8\": a 8x8 grid version\n",
    "\n",
    "The environment has two modes:\n",
    "\n",
    "* is_slippery=False: The agent always move in the intended direction due to the non-slippery nature of the frozen lake.\n",
    "* is_slippery=True: The agent may not always move in the intended direction due to the slippery nature of the frozen lake (stochastic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnAkvyzgGPqd"
   },
   "source": [
    "\n",
    "```\n",
    "# env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "```\n",
    "We can create your own custom grid like this:\n",
    "```\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JOaDf2_Giho"
   },
   "source": [
    "We need to have barriers and unsafe state in Frozen lake for the term project.\n",
    "\n",
    "First lets go with default non slippery 4x4 frozen lake and later extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXOezNCr-tlL",
    "outputId": "41ef6e5d-45df-4aca-bad9-073cdb351270"
   },
   "outputs": [],
   "source": [
    "new_step_api=True\n",
    "env1 = gym.make(\"FrozenLake-v1\", is_slippery=False ,render_mode ='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yA6HPrAl--_4",
    "outputId": "2f7f069f-25f7-4357-a536-7cf9b18d6fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env1.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0JifTyrExEp",
    "outputId": "d5cde4fd-1072-4da9-bc62-7af3bc5f1551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space : 4 | State Space: 16\n"
     ]
    }
   ],
   "source": [
    "action_size = env1.action_space.n\n",
    "state_size = env1.observation_space.n\n",
    "print(f\"Action Space : {action_size} | State Space: {state_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ked8-DCxG79E",
    "outputId": "0905e45e-e8ca-4f79-ca47-783fbbd18005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 1\n"
     ]
    }
   ],
   "source": [
    "env1.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env1.observation_space)\n",
    "print(\"Sample observation\", env1.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jct-GuzhHMaN",
    "outputId": "dc529ff9-ca6e-4430-a5a1-8334de636a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env1.action_space.n)\n",
    "print(\"Action Space Sample\", env1.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWDqPzYgHXU1"
   },
   "source": [
    "* 0: GO LEFT\n",
    "* 1: GO DOWN\n",
    "* 2: GO RIGHT\n",
    "* 3: GO UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gTuu9H6hBM2s"
   },
   "outputs": [],
   "source": [
    "env1.reset()\n",
    "env1.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jMGiVy_MLUrV"
   },
   "outputs": [],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuCjSbjnIF53"
   },
   "source": [
    "Frozen Lake Environment 8x8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-xiNxmT2_0yB"
   },
   "outputs": [],
   "source": [
    "new_step_api=True\n",
    "env2 = gym.make('FrozenLake8x8-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gv8LgDvB_-6z",
    "outputId": "4136bd6a-5596-496b-edcb-c1250ddce02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space : 4 | State Space: 64\n",
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(64)\n",
      "Sample observation 46\n",
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "source": [
    "action_size = env2.action_space.n\n",
    "state_size = env2.observation_space.n\n",
    "print(f\"Action Space : {action_size} | State Space: {state_size}\")\n",
    "\n",
    "env2.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env2.observation_space)\n",
    "print(\"Sample observation\", env2.observation_space.sample()) # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env2.action_space.n)\n",
    "print(\"Action Space Sample\", env2.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPLJ5Fp7NgQV"
   },
   "source": [
    "### Step -1 :: Google Colab Cant Render !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juIVkAz5NRRB"
   },
   "source": [
    "Trying Solution For Google Colab - Gym Envt Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUmbq3nXQGND",
    "outputId": "3c6d9c08-5abf-40f8-feee-d680274e40fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "Collecting gym-notebook-wrapper\n",
      "  Downloading gym_notebook_wrapper-1.3.2-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: ipython in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym-notebook-wrapper) (7.34.0)\n",
      "Requirement already satisfied: matplotlib in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym-notebook-wrapper) (3.5.3)\n",
      "Requirement already satisfied: gym in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym-notebook-wrapper) (0.25.2)\n",
      "Collecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym->gym-notebook-wrapper) (4.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym->gym-notebook-wrapper) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym->gym-notebook-wrapper) (1.21.6)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from gym->gym-notebook-wrapper) (0.0.8)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (0.7.5)\n",
      "Requirement already satisfied: backcall in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (3.0.31)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (63.4.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (0.1.3)\n",
      "Requirement already satisfied: pygments in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (2.13.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (5.4.0)\n",
      "Requirement already satisfied: decorator in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from ipython->gym-notebook-wrapper) (5.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (4.37.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from matplotlib->gym-notebook-wrapper) (9.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym->gym-notebook-wrapper) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym->gym-notebook-wrapper) (3.8.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from jedi>=0.16->ipython->gym-notebook-wrapper) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from pexpect>4.3->ipython->gym-notebook-wrapper) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->gym-notebook-wrapper) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sourjyadip/miniconda3/envs/gym/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->gym-notebook-wrapper) (1.16.0)\n",
      "Installing collected packages: pyvirtualdisplay, gym-notebook-wrapper\n",
      "Successfully installed gym-notebook-wrapper-1.3.2 pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!apt update && apt install xvfb\n",
    "!pip install gym-notebook-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "56rLU6ILQEcs"
   },
   "outputs": [],
   "source": [
    "import gnwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "fk24jAJcTQqg",
    "outputId": "f3d1ab12-cba0-4440-eb28-b13f82e1b31f"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Popen() missing 1 required positional argument: 'pass_fds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hs/vx5n9z_5227c54rf27b9gzhr0000gn/T/ipykernel_86321/1139663939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FrozenLake-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Start Xvfb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/gnwrapper/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, size)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mVirtual\u001b[0m \u001b[0mdisplay\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/gnwrapper/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, size)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_VirtualDisplaySingleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/gnwrapper/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subprocess.Popen\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_restart_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/pyvirtualdisplay/display.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retries_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mXStartError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m_start1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"command: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_pass_fds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_display_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\":%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m_popen\u001b[0;34m(self, use_pass_fds)\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                     \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 )\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Popen() missing 1 required positional argument: 'pass_fds'"
     ]
    }
   ],
   "source": [
    "env = gnwrapper.Animation(gym.make('FrozenLake-v1')) # Start Xvfb\n",
    "\n",
    "o = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    o, r, d, i = env.step(env.action_space.sample()) # Take action from DNN\n",
    "    env.render() # Here, clear old display and redraw new display\n",
    "    if d:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "gZP_W2PVTj75",
    "outputId": "857574f0-5445-4c65-e323-adabc03e1b21"
   },
   "outputs": [],
   "source": [
    "env = gnwrapper.LoopAnimation(gym.make('FrozenLake-v1')) # Start Xvfb\n",
    "\n",
    "o = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    o, r, d, i = env.step(env.action_space.sample()) # Take action from DNN\n",
    "    env.render() # Here, store display images\n",
    "    if d:\n",
    "        env.reset()\n",
    "\n",
    "env.display() # Display saved display images as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AlakUhMCRGa6",
    "outputId": "046aa897-57b9-4c9b-b9eb-492e94ca4fe1"
   },
   "outputs": [],
   "source": [
    "env = gnwrapper.Monitor(gym.make('FrozenLake-v1' ),directory=\"./\") # Start Xvfb\n",
    "o = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    o, r, d, i = env.step(env.action_space.sample()) # Take action from DNN\n",
    "    if d:\n",
    "        env.reset()\n",
    "\n",
    "env.display() # Here, display saved display images as movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVkoyMN0TJjo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9FVRyw-htLS"
   },
   "source": [
    "### Computing Optimal Value Function -Solving Frozen Lake Using Value Iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KB_hAgBthyOX"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env):\n",
    "\n",
    "    #set the number of iterations\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    #set the threshold number for checking the convergence of the value function\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    #we also set the discount factor\n",
    "    gamma = 1.0\n",
    "    \n",
    "    #now, we will initialize the value table, with the value of all states to zero\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
    "        #table (state values) from the previous iteration\n",
    "        updated_value_table = np.copy(value_table) \n",
    "             \n",
    "        #now, we compute the value function (state value) by taking the maximum of Q value.\n",
    "        \n",
    "        #thus, for each state, we compute the Q values of all the actions in the state and then\n",
    "        #we update the value of the state as the one which has maximum Q value as shown below:\n",
    "        for s in range(env.observation_space.n):\n",
    "            \n",
    "            Q_values = [sum([prob*(r + gamma * updated_value_table[s_])\n",
    "                             for prob, s_, r, _ in env.P[s][a]]) \n",
    "                                   for a in range(env.action_space.n)] \n",
    "                                        \n",
    "            value_table[s] = max(Q_values) \n",
    "                        \n",
    "        #after computing the value table, that is, value of all the states, we check whether the\n",
    "        #difference between value table obtained in the current iteration and previous iteration is\n",
    "        #less than or equal to a threshold value if it is less then we break the loop and return the\n",
    "        #value table as our optimal value function as shown below:\n",
    "    \n",
    "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
    "             break\n",
    "    \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFVO6zAgh9SW"
   },
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    \n",
    "    #set the discount factor\n",
    "    gamma = 1.0\n",
    "     \n",
    "    #first, we initialize the policy with zeros, that is, first, we set the actions for all the states to\n",
    "    #be zero\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    #now, we compute the Q function using the optimal value function obtained from the\n",
    "    #previous step. After computing the Q function, we can extract policy by selecting action which has\n",
    "    #maximum Q value. Since we are computing the Q function using the optimal value\n",
    "    #function, the policy extracted from the Q function will be the optimal policy. \n",
    "    \n",
    "    #As shown below, for each state, we compute the Q values for all the actions in the state and\n",
    "    #then we extract policy by selecting the action which has maximum Q value.\n",
    "    \n",
    "    #for each state\n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        #compute the Q value of all the actions in the state\n",
    "        Q_values = [sum([prob*(r + gamma * value_table[s_])\n",
    "                             for prob, s_, r, _ in env.P[s][a]]) \n",
    "                                   for a in range(env.action_space.n)] \n",
    "                \n",
    "        #extract policy by selecting the action which has maximum Q value\n",
    "        policy[s] = np.argmax(np.array(Q_values))        \n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O8MiUZeiGX9"
   },
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsYHqnAYiIjd"
   },
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6cU7fn4iSDn",
    "outputId": "6dcd6756-dc15-4ab3-da6a-c232ecc24b4f"
   },
   "outputs": [],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx7jx1ESbHRD",
    "outputId": "c02a0c73-68a1-41a8-9894-abd861275eee"
   },
   "outputs": [],
   "source": [
    "env3 = gym.make('FrozenLake-v1')\n",
    "env4 = gym.make('FrozenLake8x8-v1')\n",
    "env3.reset()\n",
    "env4.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOXARgKjbU6A",
    "outputId": "8ce44bd4-e814-44c7-ea13-875c273d2f41"
   },
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env3)\n",
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgGaejYjbhpO",
    "outputId": "8b2f0b83-2071-4c22-f573-7688ff5371fc"
   },
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env4)\n",
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjxOvdsLb1LZ",
    "outputId": "a77b5097-2a9d-48a6-b16c-d557193a6a1e"
   },
   "outputs": [],
   "source": [
    "env5 = gym.make('FrozenLake8x8-v1', is_slippery=False )\n",
    "env5.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtLpLCs_cQ7-",
    "outputId": "abbe9c63-c236-4e77-853c-b36d30b33055"
   },
   "outputs": [],
   "source": [
    "optimal_value_function = value_iteration(env=env5)\n",
    "optimal_policy = extract_policy(optimal_value_function)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs9dFH8mivI0"
   },
   "source": [
    "### Computing value function using policy - Solving Frozen Lake Using Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtGYjln2jIft"
   },
   "outputs": [],
   "source": [
    "def compute_value_function(policy):\n",
    "    \n",
    "    #now, let's define the number of iterations\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    #define the threshold value\n",
    "    threshold = 1e-20\n",
    "    \n",
    "    #set the discount factor\n",
    "    gamma = 1.0\n",
    "    \n",
    "    #now, we will initialize the value table, with the value of all states to zero\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
    "        #table (state values) from the previous iteration\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        \n",
    "\n",
    "        #thus, for each state, we select the action according to the given policy and then we update the\n",
    "        #value of the state using the selected action as shown below\n",
    "        \n",
    "        #for each state\n",
    "        for s in range(env.observation_space.n):\n",
    "            \n",
    "            #select the action in the state according to the policy\n",
    "            a = policy[s]\n",
    "            \n",
    "            #compute the value of the state using the selected action\n",
    "            value_table[s] = sum([prob * (r + gamma * updated_value_table[s_]) \n",
    "                                        for prob, s_, r, _ in env.P[s][a]])\n",
    "            \n",
    "        #after computing the value table, that is, value of all the states, we check whether the\n",
    "        #difference between value table obtained in the current iteration and previous iteration is\n",
    "        #less than or equal to a threshold value if it is less then we break the loop and return the\n",
    "        #value table as an accurate value function of the given policy\n",
    "\n",
    "        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n",
    "            break\n",
    "            \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWoic5bgjPSA"
   },
   "outputs": [],
   "source": [
    "def extract_policy(value_table):\n",
    "    \n",
    "    #set the discount factor\n",
    "    gamma = 1.0\n",
    "     \n",
    "    #first, we initialize the policy with zeros, that is, first, we set the actions for all the states to\n",
    "    #be zero\n",
    "    policy = np.zeros(env.observation_space.n) \n",
    "    \n",
    "    #now, we compute the Q function using the optimal value function obtained from the\n",
    "    #previous step. After computing the Q function, we can extract policy by selecting action which has\n",
    "    #maximum Q value. Since we are computing the Q function using the optimal value\n",
    "    #function, the policy extracted from the Q function will be the optimal policy. \n",
    "    \n",
    "    #As shown below, for each state, we compute the Q values for all the actions in the state and\n",
    "    #then we extract policy by selecting the action which has maximum Q value.\n",
    "    \n",
    "    #for each state\n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        #compute the Q value of all the actions in the state\n",
    "        Q_values = [sum([prob*(r + gamma * value_table[s_])\n",
    "                             for prob, s_, r, _ in env.P[s][a]]) \n",
    "                                   for a in range(env.action_space.n)] \n",
    "                \n",
    "        #extract policy by selecting the action which has maximum Q value\n",
    "        policy[s] = np.argmax(np.array(Q_values))        \n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undMdVtSjUrI"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "    \n",
    "    #set the number of iterations\n",
    "    num_iterations = 1000\n",
    "    \n",
    "    #we learned that in the policy iteration method, we begin by initializing a random policy.\n",
    "    #so, we will initialize the random policy which selects the action 0 in all the states\n",
    "    policy = np.zeros(env.observation_space.n)  \n",
    "    \n",
    "    #for every iteration\n",
    "    for i in range(num_iterations):\n",
    "        #compute the value function using the policy\n",
    "        value_function = compute_value_function(policy)\n",
    "        \n",
    "        #extract the new policy from the computed value function\n",
    "        new_policy = extract_policy(value_function)\n",
    "           \n",
    "        #if the policy and new_policy are same then break the loop\n",
    "        if (np.all(policy == new_policy)):\n",
    "            break\n",
    "        \n",
    "        #else, update the current policy to new_policy\n",
    "        policy = new_policy\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXBuRm2jjXwu"
   },
   "outputs": [],
   "source": [
    "optimal_policy = policy_iteration(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfu2j8W-jaIo",
    "outputId": "7dd79f10-26d1-43f2-b7b3-69e964b1bf8c"
   },
   "outputs": [],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqX1Xvocjex_"
   },
   "source": [
    "### On Policy MC for Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kydGNInzgU7x"
   },
   "outputs": [],
   "source": [
    " env = gym.make(\"FrozenLake-v1\") ## Redefine the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35EZM0oUj7S-"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Q = defaultdict(float) # Dictionary for storing the Q values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m07VvI0ekBC0"
   },
   "outputs": [],
   "source": [
    "total_return = defaultdict(float) # Dictionary for storing the total return of the state-action pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehpE4P1NkFgX"
   },
   "outputs": [],
   "source": [
    "N = defaultdict(int) # Dictionary for storing the count of the number of times a state-action pair is visited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpOm8QjzkSY0"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q):\n",
    "    \n",
    "    #set the epsilon value to 0.5\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    #sample a random value from the uniform distribution, if the sampled value is less than\n",
    "    #epsilon then we select a random action else we select the best action which has maximum Q\n",
    "    #value as shown below\n",
    "    \n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9Ijuuq2kX8E"
   },
   "outputs": [],
   "source": [
    "num_timesteps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iavMSNtkZx7"
   },
   "outputs": [],
   "source": [
    "def generate_episode(Q):\n",
    "    \n",
    "    #initialize a list for storing the episode\n",
    "    episode = []\n",
    "    \n",
    "    #initialize the state using the reset function\n",
    "    state = env.reset()\n",
    "    \n",
    "    #then for each time step\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        #select the action according to the epsilon-greedy policy\n",
    "        action = epsilon_greedy_policy(state,Q)\n",
    "        \n",
    "        #perform the selected action and store the next state information\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #store the state, action, reward in the episode list\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        #if the next state is a final state then break the loop else update the next state to the current\n",
    "        #state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcYmwlvskcEv"
   },
   "outputs": [],
   "source": [
    "num_iterations = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcFf7TXAk4c6"
   },
   "source": [
    "We learned that in the on-policy control method, we will not be given any policy as an input. So, we initialize a random policy in the first iteration and improve the policy iteratively by computing Q value. Since we extract the policy from the Q function, we don't have to explicitly define the policy. As the Q value improves the policy also improves implicitly. That is, in the first iteration we generate episode by extracting the policy (epsilon-greedy) from the initialized Q function. Over a series of iterations, we will find the optimal Q function and hence we also find the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uB-FDJ7keI7"
   },
   "outputs": [],
   "source": [
    "#for each iteration\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    #so, here we pass our initialized Q function to generate an episode\n",
    "    episode = generate_episode(Q)\n",
    "    \n",
    "    #get all the state-action pairs in the episode\n",
    "    all_state_action_pairs = [(s, a) for (s,a,r) in episode]\n",
    "    \n",
    "    #store all the rewards obtained in the episode in the rewards list\n",
    "    rewards = [r for (s,a,r) in episode]\n",
    "\n",
    "    #for each step in the episode \n",
    "    for t, (state, action, reward) in enumerate(episode):\n",
    "\n",
    "        #if the state-action pair is occurring for the first time in the episode\n",
    "        if not (state, action) in all_state_action_pairs[0:t]:\n",
    "            \n",
    "            #compute the return R of the state-action pair as the sum of rewards\n",
    "            R = sum(rewards[t:])\n",
    "            \n",
    "            #update total return of the state-action pair\n",
    "            total_return[(state,action)] = total_return[(state,action)] + R\n",
    "            \n",
    "            #update the number of times the state-action pair is visited\n",
    "            N[(state, action)] += 1\n",
    "\n",
    "            #compute the Q value by just taking the average\n",
    "            Q[(state,action)] = total_return[(state, action)] / N[(state, action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXjaF9FWk8vo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "5QdwTH67lO5v",
    "outputId": "a8c78c7c-75d5-43c9-e83d-b4d7eed458e6"
   },
   "outputs": [],
   "source": [
    "df.head(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "527S8Hiyfst2",
    "outputId": "8966c5c6-49fc-4b60-99b6-025632b54fbc"
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(num_timesteps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax([Q[(state,a)] for a in range(env.action_space.n)])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal 🏆\")\n",
    "            else:\n",
    "                print(\"We fell into a hole ☠️\")\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            \n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b1HAu2lmCx2"
   },
   "source": [
    "### Temporal Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeTqrDWIl4AP"
   },
   "source": [
    "As we can observe, we have the Q values for all the state-action pairs. Now we can extract the policy by selecting the action which has maximum Q value in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKe1jnOFl5J_"
   },
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return env1.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiWOJYs6mN1j"
   },
   "outputs": [],
   "source": [
    "V = {}\n",
    "for s in range(env1.observation_space.n):\n",
    "    V[s] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwaoZ6zVmSue"
   },
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "gamma = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXIbT7IbmYCG"
   },
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "num_timesteps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psCE0SL3me-f"
   },
   "source": [
    "Let's compute the value function (state values) using the given random policy as:\n",
    "![Screenshot from 2022-09-17 00-10-36.png](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z32Nh_vHmYr2",
    "outputId": "0dbc11dd-2127-440e-ba6a-410d81f35cf4"
   },
   "outputs": [],
   "source": [
    "#for each episode\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #initialize the state by resetting the environment\n",
    "    s = env.reset()\n",
    "    \n",
    "    #for every step in the episode\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        #select an action according to random policy\n",
    "        a = random_policy()\n",
    "        \n",
    "        #perform the selected action and store the next state information\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        \n",
    "        #compute the value of the state\n",
    "        V[s] += alpha * (r + gamma * V[s_]-V[s])\n",
    "        \n",
    "        #update next state to the current state\n",
    "        s = s_\n",
    "        \n",
    "        #if the current state is the terminal state then break\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhCLRPsLmo9V"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(V.items()), columns=['state', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "P7-5Q2GGmtR3",
    "outputId": "383c6923-3d01-432b-d204-55b3988b764c"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYkIuQzTm3lm"
   },
   "source": [
    "As we can observe, now we have the value of all the states and also we can notice that the value of all the terminal states (hole states and goal state) is zero.\n",
    "\n",
    "Now that we have understood how TD learning can be used for the prediction task, in the next section, we will learn how to use TD learning for the control task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IrZVgXcm5eu"
   },
   "source": [
    "### TD For Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_-OSvwSnMNt"
   },
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcWLIY5InS8E"
   },
   "source": [
    "Let's define the dictionary for storing the Q value of the state-action pair and we initialize the Q value of all the state-action pair to 0.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V6abcTXm8hM"
   },
   "outputs": [],
   "source": [
    "Q = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s,a)] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beeL6oefnYmk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM4QivdLnWrz"
   },
   "source": [
    "Now, let's define the epsilon-greedy policy. We generate a random number from the uniform distribution and if the random number is less than epsilon we select the random action else we select the best action which has the maximum Q value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcXIflSUnZWg"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG9ILgyKnbRs"
   },
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "gamma = 0.90\n",
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfI-vkq6ndfs"
   },
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "num_timesteps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHLuyKZCnhW0"
   },
   "source": [
    "C![Screenshot from 2022-09-17 00-15-01.png](data:image/png;base64,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)ompute the optimal policy using the SARSA update rule as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38QgMDzDnfVy",
    "outputId": "8743b90d-c260-4f38-cfb0-a44e38520aa9"
   },
   "outputs": [],
   "source": [
    "#for each episode\n",
    "for i in range(20000):\n",
    "       \n",
    "    #initialize the state by resetting the environment\n",
    "    s = env.reset()\n",
    "    \n",
    "    #select the action using the epsilon-greedy policy\n",
    "    a = epsilon_greedy(s,epsilon)\n",
    "    \n",
    "    #for each step in the episode:\n",
    "    for t in range(num_timesteps):\n",
    "\n",
    "        #perform the selected action and store the next state information: \n",
    "        s_, r, done, _ = env.step(a)\n",
    "        \n",
    "        #select the action a dash in the next state using the epsilon greedy policy:\n",
    "        a_ = epsilon_greedy(s_,epsilon) \n",
    "        \n",
    "        #compute the Q value of the state-action pair\n",
    "        Q[(s,a)] += alpha * (r + gamma * Q[(s_,a_)]-Q[(s,a)])\n",
    "        \n",
    "        #update next state to current state\n",
    "        s = s_\n",
    "        \n",
    "        #update next action to current action\n",
    "        a = a_\n",
    "\n",
    "\n",
    "        #if the current state is the terminal state then break:\n",
    "        if done:\n",
    "            break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywRfS-wfn6Xb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkLaQGL5n7yG"
   },
   "source": [
    "Note that on every iteration we update the Q function. After all the iterations, we will have the optimal Q function. Once we have the optimal Q function then we can extract the optimal policy by selecting the action which has maximum Q value in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "TgwaRKdyn8Db",
    "outputId": "19570b53-bc19-4bd9-93f5-c5adcc9ad189"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZWHRs4yoBW5"
   },
   "source": [
    "#### Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TDFV-SaoZmb"
   },
   "source": [
    "Let's define the dictionary for storing the Q value of the state-action pair and we initialize the Q value of all the state-action pair to 0.0:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHhAcLVwoFFy"
   },
   "outputs": [],
   "source": [
    "Q = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        Q[(s,a)] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mWyDRqvoe96"
   },
   "source": [
    "\n",
    "Now, let's define the epsilon-greedy policy. We generate a random number from the uniform distribution and if the random number is less than epsilon we select the random action else we select the best action which has the maximum Q value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNZxjcOUokoh"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbZKuBR9omk4"
   },
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "gamma = 0.90\n",
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twGT_QJhopwh"
   },
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWlo9ikLotXj"
   },
   "source": [
    "Compute the optimal policy using the Q learning update rule as: ![Screenshot from 2022-09-17 00-20-12.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VcQeIX6oq5S",
    "outputId": "b151cb7d-6ad6-490e-b0d8-ca18882da6da"
   },
   "outputs": [],
   "source": [
    "#for each episode:\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    #initialize the state by resetting the environment\n",
    "    s = env.reset()\n",
    "    \n",
    "    #for each step in the episode\n",
    "    for t in range(num_steps):\n",
    "        \n",
    "        #select the action using the epsilon-greedy policy\n",
    "        a = epsilon_greedy(s,epsilon)\n",
    "        \n",
    "        #perform the selected action and store the next state information\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        \n",
    "        #first, select the action a dash which has a maximum Q value in the next state\n",
    "        a_ = np.argmax([Q[(s_, a)] for a in range(env.action_space.n)])\n",
    "    \n",
    "        # we calculate the Q value of previous state using our update rule\n",
    "        Q[(s,a)] += alpha * (r + gamma * Q[(s_,a_)]-Q[(s,a)])\n",
    "    \n",
    "        #update current state to next state\n",
    "        s = s_\n",
    "        \n",
    "        #if the current state is the terminal state then break  \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Jlmo3vXpozi6",
    "outputId": "738b39ef-79e4-4957-87a7-03da5910d691"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
